{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import dlc_practical_prologue as prologue\n",
    "from BaseNet import *\n",
    "from ConvNet1 import *\n",
    "\n",
    "######################################################\n",
    "### FRAMEWORK FOR INPUT AS ONE IMAGE OF 2 CHANNELS ###\n",
    "\n",
    "# In this framework, the network is trained to directly predict if the first digit is less or equal to the second. It doesn't learn to recognize the digits.\n",
    "\n",
    "nb_classes = 1\n",
    "nb_input_channels = 2\n",
    "\n",
    "mini_batch_size = 1000\n",
    "nb_epochs = 300\n",
    "eta = 0.001\n",
    "\n",
    "\n",
    "# Network is a classifier: it treats the pairs as 2 images of 1 channel and is trained to predict the digit from the image\n",
    "def train_model_2C(model, train_input, train_target, optimizer, mini_batch_size=mini_batch_size,\n",
    "                   criterion=torch.nn.BCEWithLogitsLoss(), nb_epochs=nb_epochs):\n",
    "    train_target = train_target.type(torch.FloatTensor).view(-1, 1)\n",
    "    nb_samples = len(train_input)\n",
    "    \n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for e in range(0, nb_epochs):\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train': model.train()\n",
    "            else: model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for b in range(0, train_input.size(0), mini_batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                    #print(\"output = \", output)\n",
    "                    target = train_target.narrow(0, b, mini_batch_size)\n",
    "                    \n",
    "                    # the nn.CrossEntropyLoss expects a class index as the target for each value\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * train_input.size(0)\n",
    "                #running_corrects += torch.sum(torch.max(output, 1)[1] == target)       \n",
    "\n",
    "            epoch_loss = running_loss / nb_samples\n",
    "            #epoch_acc = running_corrects.double() / nb_samples\n",
    "            epoch_acc = 0\n",
    "            \n",
    "            if (e % 100 == 99):\n",
    "                print('phase: %s, epoch: %d, loss: %.5f, acc: %.4f' %\n",
    "                      (phase, e+1, epoch_loss, epoch_acc))\n",
    "                \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in %.0f min %.0f s' % (time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: %.4f' % (best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "def test_model_2C(model, test_input, test_target):  \n",
    "    # Number of pairs incorrectly identified\n",
    "    test_output = model(test_input)\n",
    "    m = nn.Sigmoid()\n",
    "    print(\"test_output = \", m(test_output)[:10])\n",
    "    test_output_results = torch.le(m(test_output), 0.5).type(torch.LongTensor)\n",
    "    print(\"test_output_results = \", test_output_results[:10])\n",
    "    print(\"test_target = \", test_target[:10])\n",
    "    nb_errors_pairs = torch.abs(test_output_results-test_target).sum().item()\n",
    "    print(\"nb_errors_pairs = \", nb_errors_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train, epoch: 100, loss: 0.18176, acc: 0.0000\n",
      "phase: val, epoch: 100, loss: 0.17862, acc: 0.0000\n",
      "phase: train, epoch: 200, loss: 0.01631, acc: 0.0000\n",
      "phase: val, epoch: 200, loss: 0.01598, acc: 0.0000\n",
      "phase: train, epoch: 300, loss: 0.00434, acc: 0.0000\n",
      "phase: val, epoch: 300, loss: 0.00430, acc: 0.0000\n",
      "Training complete in 1 min 14 s\n",
      "Best val acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = \\\n",
    "    prologue.generate_pair_sets(nb=1000)\n",
    "\n",
    "#model = BaseNet2C(nb_classes)\n",
    "model = ConvNet1_2C(nb_classes)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=eta, momentum=0.95)\n",
    "model, val_acc_history = train_model_2C(model, train_input, train_target, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output =  tensor([[1.0000],\n",
      "        [0.9017],\n",
      "        [0.9987],\n",
      "        [0.9998],\n",
      "        [0.9823],\n",
      "        [0.9238],\n",
      "        [0.9882],\n",
      "        [0.9997],\n",
      "        [0.9843],\n",
      "        [0.9998]], grad_fn=<SliceBackward>)\n",
      "test_output_results =  tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "test_target =  tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1])\n",
      "nb_errors_pairs =  523504\n"
     ]
    }
   ],
   "source": [
    "test_model_2C(model, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 errors in the training dataset\n",
      "There are 0 errors in the test dataset\n"
     ]
    }
   ],
   "source": [
    "# Computes the number of errors between target and classes\n",
    "def compute_nb_errors_data(set_, target, classes): \n",
    "    is_pair = torch.le(classes[:,0], classes[:,1]).type(torch.LongTensor)\n",
    "    nb_errors = torch.abs(target-is_pair).sum().item()\n",
    "    print('There are %d errors in the %s dataset' % (nb_errors, set_))\n",
    "    \n",
    "compute_nb_errors_data('training', train_target, train_classes)\n",
    "compute_nb_errors_data('test', test_target, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
