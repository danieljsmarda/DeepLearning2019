{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\" Base class \"\"\"\n",
    "    def __init__(self):\n",
    "        self._parameters = dict()\n",
    "        self._children = dict()\n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \"\"\" backward receives as input a pointer to a tensor or a tuple of tensors containing\n",
    "        the gradient of the loss (or the function of interest) wrt the module's output, accumulates\n",
    "        the gradient wrt the parameters, and returns a tensor or a tuple of tensors containing the \n",
    "        gradient of the loss wrt the module's input (Application of the chain rule)\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_children(self, name, module):\n",
    "        assert isinstance(module, Module) and is not None, \"Not a Module.\"\n",
    "        assert name not in self._children, \"Module {} already exists\".format(name)\n",
    "        self._children[name] = module\n",
    "        \n",
    "    def add_parameter(self, name, param):\n",
    "        assert isinstance(param, Parameter), \"Not a Parameter.\"\n",
    "        assert name not in self._parameters, \"Parameter {} already exists\".format(name)\n",
    "        self._parameters[name] = param\n",
    "        \n",
    "    def param(self):\n",
    "        return self._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(torch.Tensor):\n",
    "    def __init__(self, tensor=None, grad=None, requires_grad=True):\n",
    "        self.tensor = tensor\n",
    "        self.grad = None\n",
    "        self.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\" Implements a R^C -> R^D fully-connected layer:\n",
    "        Input: (N x C) tensor\n",
    "        Ouput: (N x D) tensor \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "            \n",
    "    def init_parameters(self):\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        output = torch.matmul(input, self.weight.t())\n",
    "        if bias: output += bias\n",
    "        return output\n",
    "              \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_bachward\n",
    "        grad_input = torch.matmul(grad_output, self.weight)\n",
    "        grad_weight = torch.matmul(grad_output.t(), input)\n",
    "        self.weight.gradient += grad_weight\n",
    "        if self.bias: \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "            self.bias.grad += grad_bias\n",
    "        return grad_input    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second project, do we first accumulate the gradient then afterwards calculate the derivate of the loss wrt \n",
    "to the input.  Or do it the other way around.\n",
    "They are usually unrelated computations. Think about the following scenario. You have a batch of inputs x_0 to x_9. \n",
    "And a single parameter a. Thus the forward pass for this module is s_i = a*x_i. For the backward pass we get as \n",
    "input dl/ds_i for all i and we need to compute dl/da and dl/dx_i . It is quite obvious that \n",
    "dl/da = sum x_i * dl/ds_i for all i. And dl/dx_i = dl/ds_i * a. The order in which one computes the two is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING PARAMETERS\n",
      "p =  Parameter containing:\n",
      "tensor([[-0.6022,  0.5215]], requires_grad=True)\n",
      "p =  Parameter containing:\n",
      "tensor([-0.7021], requires_grad=True)\n",
      "PRINTING PREDICTION\n",
      "y_pred =  tensor([[-0.2612],\n",
      "        [-1.3850],\n",
      "        [-0.4226]], grad_fn=<AddmmBackward>)\n",
      "PRINTING GRADIENT\n",
      "p.grad =  tensor([[-8.7326, -9.2429]])\n",
      "p.grad =  tensor([-4.3125])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = torch.tensor([[1, 2], [2, 1], [3, 4]]).type(torch.FloatTensor).requires_grad_()\n",
    "y = torch.tensor([1, 0.4, 3])\n",
    "#x = torch.tensor([[1., 2.]]).requires_grad_()\n",
    "#y = torch.tensor([1.])\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 1))\n",
    "print(\"PRINTING PARAMETERS\")\n",
    "for p in model.parameters():\n",
    "    print(\"p = \", p)\n",
    "y_pred = model(x)\n",
    "\n",
    "print(\"PRINTING PREDICTION\")\n",
    "print(\"y_pred = \", y_pred)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss = criterion(y_pred, y)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"PRINTING GRADIENT\")\n",
    "#print(\"loss.grad = \", autograd.grad(loss, x))\n",
    "loss.backward()\n",
    "for p in model.parameters():\n",
    "    print(\"p.grad = \", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
